{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=ToTensor(), \n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000, 10000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) torch.float32 9\n"
     ]
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "print(image.shape, image.dtype, label) #NCHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '9')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "plt.title(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "Train dataloader: 1875 batches of 32\n",
      "Test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, \n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True\n",
    ")\n",
    "test_dataloader = DataLoader(test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             shuffle=False\n",
    ")\n",
    "print(len(train_dataloader.dataset))\n",
    "print(f\"Train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.Size([32]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NCHW\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "train_features_batch.shape, train_labels_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 0 \n",
    "class VanillaModel(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features will be H X W\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n",
    "class NonLinearModel(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    tp_tn = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (tp_tn / len(y_pred)) * 100 \n",
    "    return acc\n",
    "\n",
    "\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    torch.mps.synchronize()\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.2f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VanillaModel(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = VanillaModel(input_shape=image.shape[1]*image.shape[2],\n",
    "                       hidden_units=10,\n",
    "                       output_shape=len(train_data.classes)\n",
    ")\n",
    "model_0.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(str(next(model_0.parameters()).device))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:14,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:13,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:04<00:11,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:06<00:09,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:08<00:08,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:09<00:06,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:11<00:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:13<00:03,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:14<00:01,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:16<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.40684 | Testing loss: 0.48635, Test acc: 82.22%\n",
      "\n",
      "Train time on cpu: 16.27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_time_start = timer()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "\n",
    "    # Training\n",
    "    train_loss = 0 \n",
    "    \n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train() \n",
    "    \n",
    "        y_pred = model_0(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    # Testing\n",
    "    test_loss = 0\n",
    "    test_acc = 0 \n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            \n",
    "            test_pred = model_0(X)\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += compute_accuracy(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # per batch\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    print(f\"\\nTraining loss: {train_loss:.5f} | Testing loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "\n",
    "train_time_end = timer()\n",
    "train_time_model_0 = print_train_time(start=train_time_start, \n",
    "                                      end=train_time_end,\n",
    "                                      device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: torch.nn.Module, \n",
    "                   data_loader: torch.utils.data.DataLoader, \n",
    "                   loss_fn: torch.nn.Module, \n",
    "                   accuracy_fn,\n",
    "                   device: torch.device = device):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # send data to the target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "        \n",
    "        # per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__,\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'VanillaModel',\n",
       " 'model_loss': 0.4863521456718445,\n",
       " 'model_accuracy': 82.21845047923323}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 0 results on test set\n",
    "model_0_results = evaluate_model(model=model_0, \n",
    "                                 data_loader=test_dataloader,\n",
    "                                 loss_fn=loss_fn, \n",
    "                                 accuracy_fn=compute_accuracy,\n",
    "                                 device=device\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# using M chip Mac\n",
    "if torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "   x = torch.ones(1, device=device)\n",
    "   print(x)\n",
    "else:\n",
    "   print(\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_1 = NonLinearModel(input_shape=image.shape[1]*image.shape[2],\n",
    "                         hidden_units=10,\n",
    "                         output_shape=len(train_data.classes)\n",
    ").to(device) # send model to GPU if it's available\n",
    "\n",
    "print(str(next(model_1.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: torch.nn.Module,\n",
    "                data_loader: torch.utils.data.DataLoader,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                accuracy_fn,\n",
    "                device: torch.device = device):\n",
    "\n",
    "    # Training\n",
    "    train_loss = 0 \n",
    "    train_acc = 0\n",
    "    model.to(device)\n",
    "\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # send data to GPU\n",
    "        X, y = X.to(device), y.to(device)\n",
    "    \n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(data_loader)\n",
    "\n",
    "    print(f\"Training loss: {train_loss:.5f} | Training accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "\n",
    "def test_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    \n",
    "    # Testing\n",
    "    test_loss = 0\n",
    "    test_acc = 0 \n",
    "    model.to(device)\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # send data to GPU\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            test_pred = model(X)\n",
    "            test_loss += loss_fn(test_pred, y)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        # per batch\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    print(f\"Testing loss: {test_loss:.5f} | Testing accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Training loss: 0.84214 | Training accuracy: 70.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:56,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.71671 | Testing accuracy: 73.79%\n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:11<00:47,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.67697 | Training accuracy: 75.44%\n",
      "Epoch: 2\n",
      "Training loss: 0.65150 | Training accuracy: 76.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:17<00:41,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.66602 | Testing accuracy: 75.82%\n",
      "\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:23<00:34,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.63573 | Training accuracy: 76.72%\n",
      "Epoch: 4\n",
      "Training loss: 0.62549 | Training accuracy: 77.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:29<00:29,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.65772 | Testing accuracy: 75.78%\n",
      "\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:35<00:23,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.61992 | Training accuracy: 77.16%\n",
      "Epoch: 6\n",
      "Training loss: 0.61418 | Training accuracy: 77.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:41<00:17,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.64065 | Testing accuracy: 76.59%\n",
      "\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:47<00:11,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.60846 | Training accuracy: 77.46%\n",
      "Epoch: 8\n",
      "Training loss: 0.60431 | Training accuracy: 77.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:53<00:05,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss: 0.64988 | Testing accuracy: 76.40%\n",
      "\n",
      "Epoch: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:59<00:00,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.60098 | Training accuracy: 77.81%\n",
      "Train time on mps: 59.09 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_time_start = timer()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "\n",
    "    train_model(data_loader=train_dataloader, \n",
    "                model=model_1, \n",
    "                loss_fn=loss_fn,\n",
    "                optimizer=optimizer,\n",
    "                accuracy_fn=compute_accuracy\n",
    "    )\n",
    "    if epoch % 2 == 0:\n",
    "        test_model(model=model_1,\n",
    "                data_loader=test_dataloader,\n",
    "                loss_fn=loss_fn,\n",
    "                accuracy_fn=compute_accuracy\n",
    "        )\n",
    "\n",
    "train_time_end = timer()\n",
    "train_time_model_1 = print_train_time(start=train_time_start, \n",
    "                                      end=train_time_end,\n",
    "                                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 results on test set\n",
    "model_1_results = evaluate_model(model=model_1, \n",
    "                                 data_loader=test_dataloader,\n",
    "                                 loss_fn=loss_fn, \n",
    "                                 accuracy_fn=compute_accuracy,\n",
    "                                 device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'VanillaModel', 'model_loss': 0.4863521456718445, 'model_accuracy': 82.21845047923323}\n",
      "{'model_name': 'NonLinearModel', 'model_loss': 0.6520592570304871, 'model_accuracy': 76.15814696485623}\n"
     ]
    }
   ],
   "source": [
    "print(model_0_results)\n",
    "print(model_1_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN - TinyVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "torch.Size([1, 10, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "test_images = torch.randn(size=(32, 3, 64, 64))\n",
    "test_image = test_images[0]\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                         out_channels=10,\n",
    "                         kernel_size=(5, 5),\n",
    "                         stride=2,\n",
    "                         padding=0)\n",
    "\n",
    "print(test_image.shape)\n",
    "print(conv_layer(test_image.unsqueeze(dim=0)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[[[-1.0788e-01,  5.9426e-02,  1.1359e-03,  6.1827e-03, -1.4073e-02],\n",
      "          [-1.6203e-02,  5.4726e-02, -7.6181e-02,  9.3466e-02, -8.5709e-02],\n",
      "          [ 3.1117e-02,  4.7625e-02,  4.4068e-02,  5.2393e-02,  9.9105e-02],\n",
      "          [-6.3450e-02,  1.1040e-01,  6.3507e-03,  6.3543e-02,  7.5569e-02],\n",
      "          [ 1.7745e-02,  4.6209e-02,  7.0248e-02,  1.6617e-02,  1.0131e-01]],\n",
      "\n",
      "         [[-2.6650e-02, -2.9258e-02,  3.3528e-03,  7.2465e-02,  6.8792e-02],\n",
      "          [-8.5270e-02, -8.6530e-02,  4.1514e-02,  1.0900e-03,  6.0747e-02],\n",
      "          [ 8.2494e-02,  9.1270e-02, -4.1793e-02, -6.9249e-02, -5.9285e-02],\n",
      "          [-1.1438e-01, -5.1687e-03, -9.7562e-02, -7.5620e-02,  2.3226e-02],\n",
      "          [-3.2276e-05,  6.9110e-03, -6.9991e-02, -2.9666e-02, -9.6323e-02]],\n",
      "\n",
      "         [[-1.1511e-01,  9.6922e-02, -8.2784e-02, -9.7121e-02,  2.8138e-02],\n",
      "          [-1.0416e-01,  1.0653e-01, -8.3826e-02,  7.3642e-02,  6.7204e-02],\n",
      "          [ 4.4635e-02, -6.4581e-02, -9.3340e-02, -5.5399e-02, -1.3735e-03],\n",
      "          [-7.3216e-02, -5.4274e-02,  5.0614e-02, -1.1226e-01, -1.2386e-02],\n",
      "          [ 4.9235e-02,  7.7173e-02, -1.1061e-01,  3.1477e-02,  5.8520e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.8361e-02,  7.9545e-02, -6.4543e-02, -6.1902e-02, -1.6110e-02],\n",
      "          [ 7.6549e-02, -6.7368e-02,  6.0869e-02, -6.2277e-02,  9.0747e-02],\n",
      "          [-6.2605e-03, -5.3923e-02, -9.0258e-02, -3.1461e-02,  1.0073e-01],\n",
      "          [-6.8564e-02, -6.5371e-05,  1.1182e-01,  9.0652e-02, -5.7011e-02],\n",
      "          [-9.8323e-02,  4.9687e-02,  9.0129e-02,  8.7287e-02, -6.2629e-02]],\n",
      "\n",
      "         [[ 3.7095e-02, -6.6096e-02,  9.1836e-02,  1.1500e-01, -9.8289e-02],\n",
      "          [ 1.0752e-01, -5.0104e-02,  1.3766e-03, -6.4650e-02, -8.8854e-02],\n",
      "          [ 5.0672e-02,  9.5470e-03,  2.3063e-02,  2.1787e-02, -8.6597e-02],\n",
      "          [-7.1475e-03, -4.2611e-02, -8.7920e-02, -8.9114e-02, -5.2546e-02],\n",
      "          [-8.4142e-02, -1.0808e-01,  8.7742e-02,  1.0833e-01,  8.0607e-02]],\n",
      "\n",
      "         [[-7.2330e-03, -1.3285e-02,  2.1425e-02, -9.4364e-02, -8.8508e-02],\n",
      "          [ 1.9428e-02, -6.0294e-02, -8.9351e-02, -1.9942e-02,  3.4801e-02],\n",
      "          [ 4.0576e-02, -9.1140e-02, -1.0630e-01,  1.2102e-02, -4.6561e-02],\n",
      "          [ 7.4935e-02, -1.1034e-01, -4.8229e-02,  4.4782e-02,  6.6985e-02],\n",
      "          [ 2.1246e-02,  3.5955e-02, -2.1048e-02,  6.4956e-02,  8.2107e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5768e-02,  5.1388e-02,  9.1836e-02,  1.1459e-01, -5.4391e-03],\n",
      "          [ 1.1214e-03, -2.1093e-02, -9.1900e-02,  9.1988e-03, -2.8316e-02],\n",
      "          [ 1.2514e-02, -7.8003e-02,  5.1637e-02,  1.6559e-02, -1.5594e-02],\n",
      "          [ 1.2762e-02,  9.0582e-02,  8.3035e-02, -2.3611e-02, -1.1117e-01],\n",
      "          [-2.2820e-02,  5.7690e-02, -6.2986e-03, -1.0852e-01, -9.8409e-02]],\n",
      "\n",
      "         [[ 7.6483e-02,  2.4073e-02,  3.6848e-02, -4.2991e-02, -4.1704e-02],\n",
      "          [ 1.7560e-02,  1.0143e-01,  2.9476e-02, -3.0840e-02, -6.1935e-02],\n",
      "          [-3.6079e-02, -8.3238e-02,  1.8990e-02,  8.7029e-02,  6.9077e-03],\n",
      "          [-1.2425e-02, -9.5458e-02, -2.9893e-03, -5.7799e-02, -1.0154e-02],\n",
      "          [ 2.7447e-02, -3.4900e-02, -6.0100e-02,  4.4248e-02,  9.5439e-02]],\n",
      "\n",
      "         [[ 1.1381e-01, -4.9984e-02,  6.1569e-02,  7.8087e-02,  8.3835e-02],\n",
      "          [-1.0768e-01, -4.7838e-02, -5.2710e-02, -1.0397e-01,  1.1347e-01],\n",
      "          [ 3.7639e-02,  7.0009e-02, -6.4258e-02,  1.0128e-01, -3.3997e-02],\n",
      "          [-1.3271e-02,  9.5741e-02,  5.2109e-02,  9.1140e-02,  1.1133e-01],\n",
      "          [-5.0976e-02, -2.1105e-02, -1.0626e-01,  7.4446e-02, -4.5531e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.9808e-03,  7.0541e-02,  4.8624e-02,  7.3629e-02,  5.7655e-02],\n",
      "          [-1.0356e-01,  2.4986e-02,  1.1072e-01, -1.0783e-02, -6.8637e-03],\n",
      "          [ 4.4054e-03,  2.1358e-02, -2.2761e-02,  1.2130e-02, -6.5143e-03],\n",
      "          [ 2.1280e-02, -6.2950e-02,  4.8796e-02,  3.7879e-02, -7.9337e-02],\n",
      "          [ 1.0004e-01,  7.5248e-02, -4.9023e-02, -8.8127e-02,  5.2109e-02]],\n",
      "\n",
      "         [[ 5.7914e-02,  3.7348e-02,  1.0095e-01,  1.2307e-02,  8.1939e-02],\n",
      "          [-1.0238e-01,  1.0469e-02,  9.3040e-02,  2.9952e-02, -7.9510e-02],\n",
      "          [-7.7387e-02,  1.0686e-01, -7.6858e-02, -5.9393e-02, -6.0629e-02],\n",
      "          [ 4.3097e-03,  4.5899e-02, -2.1376e-02,  5.3747e-02, -1.5207e-02],\n",
      "          [-1.9660e-02,  1.7149e-02, -8.7077e-02,  3.4818e-03,  2.4030e-02]],\n",
      "\n",
      "         [[ 1.0412e-01, -7.2813e-02,  7.5712e-02,  2.1949e-02, -3.9228e-02],\n",
      "          [ 4.3232e-02,  7.8996e-02,  2.6131e-02,  1.9232e-02,  1.0357e-01],\n",
      "          [ 4.6370e-02, -5.6519e-02,  7.4574e-02, -9.7344e-02, -3.9989e-02],\n",
      "          [ 2.8307e-02, -8.1991e-02, -4.9235e-02,  9.0459e-02,  9.0173e-02],\n",
      "          [ 4.1997e-02,  1.1016e-02,  1.0660e-01,  7.6283e-03,  3.0389e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0650e-01, -2.8812e-02,  5.3858e-02, -3.9356e-02, -7.3335e-03],\n",
      "          [ 8.0163e-02,  3.2518e-02, -3.1353e-02, -1.0156e-01, -4.0989e-02],\n",
      "          [-1.1364e-01, -3.1552e-02, -3.9568e-03,  1.0848e-01, -8.6143e-02],\n",
      "          [ 6.7590e-02,  5.4244e-02, -3.3199e-02, -9.4361e-02,  7.6651e-02],\n",
      "          [-8.9030e-02,  5.6297e-02, -6.9628e-02, -3.1957e-02, -3.6567e-02]],\n",
      "\n",
      "         [[-6.8335e-02,  8.4381e-02,  1.3970e-02,  8.3773e-02,  4.6086e-02],\n",
      "          [-1.4106e-02,  9.1290e-02, -2.8715e-02,  2.7988e-02, -3.4803e-02],\n",
      "          [-6.0853e-02, -2.8674e-02,  5.9326e-03, -8.0069e-02, -7.3164e-02],\n",
      "          [ 1.0615e-01,  1.0238e-01,  7.5119e-02, -6.6876e-02, -2.5591e-02],\n",
      "          [-1.0753e-01,  4.1453e-02, -2.4253e-03, -6.1347e-02, -1.3680e-02]],\n",
      "\n",
      "         [[ 9.9858e-02,  7.3776e-02, -3.5863e-02, -6.0818e-02, -1.8064e-02],\n",
      "          [ 8.0108e-02,  7.0629e-02,  6.8990e-02, -5.9909e-02,  8.8164e-02],\n",
      "          [ 5.8260e-02, -2.1006e-02, -8.0251e-02,  7.7773e-02,  7.1988e-02],\n",
      "          [-7.0592e-02, -7.8632e-02, -6.2091e-03, -9.7066e-02,  6.7556e-02],\n",
      "          [-8.9924e-02,  1.0051e-01,  5.0832e-02,  9.7287e-02,  7.8890e-03]]],\n",
      "\n",
      "\n",
      "        [[[-7.7458e-03, -3.0678e-02, -4.8270e-03, -1.0016e-01, -2.7631e-02],\n",
      "          [ 2.1887e-02,  8.6465e-02, -8.1064e-02,  6.9723e-02,  9.4452e-02],\n",
      "          [-3.0181e-02, -5.3827e-02, -7.6035e-02, -2.2717e-02,  2.9053e-02],\n",
      "          [-9.3725e-02, -2.9715e-03,  7.2544e-02, -6.5001e-02, -4.9188e-03],\n",
      "          [-4.8003e-02,  8.9482e-02, -3.2028e-02, -8.9463e-02,  1.0495e-02]],\n",
      "\n",
      "         [[ 1.4801e-02,  1.1321e-01, -8.8482e-02, -8.8252e-02,  2.1255e-02],\n",
      "          [-2.5757e-02, -6.1946e-02,  7.4298e-02,  8.9275e-02, -2.4047e-03],\n",
      "          [-2.2421e-02,  1.1178e-01, -7.0014e-02, -7.1197e-02,  8.0011e-02],\n",
      "          [ 7.1084e-02, -1.0600e-02, -3.7689e-02,  9.0086e-02, -1.6904e-02],\n",
      "          [ 1.7891e-02,  7.7190e-02, -1.1062e-01,  1.1210e-01,  1.0017e-01]],\n",
      "\n",
      "         [[-2.2340e-02, -1.0355e-01, -7.4383e-02,  5.1755e-02, -7.9793e-02],\n",
      "          [ 9.4905e-02, -1.0644e-01,  2.4560e-02, -4.9096e-02,  5.9794e-02],\n",
      "          [-1.0939e-01, -1.0478e-01,  8.1290e-03,  8.3824e-02, -6.1473e-02],\n",
      "          [ 1.1226e-01, -5.1410e-02,  4.8834e-02, -4.2176e-02, -1.1488e-01],\n",
      "          [-9.1227e-02,  9.7101e-02,  7.2650e-02, -4.7155e-02, -7.4273e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7095e-02,  4.5120e-02, -3.5348e-02,  4.8790e-02, -1.7586e-02],\n",
      "          [-6.1423e-02,  5.1943e-02, -1.0078e-01,  4.7620e-02,  9.2556e-02],\n",
      "          [ 1.1326e-02,  1.1486e-01,  4.5314e-02, -7.2677e-02,  1.0504e-01],\n",
      "          [ 5.4332e-02,  1.9104e-02, -1.1621e-02,  1.1242e-01, -4.7403e-02],\n",
      "          [-6.6316e-02,  1.1157e-01, -6.4038e-02,  4.2619e-03,  1.0472e-01]],\n",
      "\n",
      "         [[-8.9775e-02, -1.2739e-02, -6.7631e-02,  7.4028e-03,  1.0277e-01],\n",
      "          [-1.3049e-02,  5.8508e-02, -7.0298e-02,  8.2637e-02, -7.7641e-02],\n",
      "          [-1.1068e-01, -2.8992e-02,  3.0552e-02, -4.7113e-02,  8.5699e-02],\n",
      "          [-1.0978e-01,  9.2840e-02, -1.2889e-02, -4.1011e-02,  1.0579e-01],\n",
      "          [-2.8973e-02, -3.0809e-02, -1.1402e-01,  1.0835e-01, -5.3805e-03]],\n",
      "\n",
      "         [[ 9.1628e-02,  1.2132e-02,  9.9359e-02, -7.4719e-02, -5.1673e-02],\n",
      "          [ 8.8152e-02, -6.3538e-02, -4.7063e-02,  1.0660e-01,  2.8752e-02],\n",
      "          [ 4.3558e-02, -1.0352e-01,  6.3963e-02,  6.3353e-02,  1.1176e-01],\n",
      "          [-7.5757e-02, -4.4791e-02, -1.3920e-02,  3.8261e-03,  8.2299e-02],\n",
      "          [-1.0333e-01,  1.0461e-03,  5.5081e-02, -8.7104e-02,  9.9826e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8284e-02,  1.1522e-01, -1.1464e-01,  9.4012e-02, -2.7309e-02],\n",
      "          [-9.8724e-02, -5.1271e-02, -9.4376e-02,  1.0205e-01, -9.1614e-02],\n",
      "          [ 4.4849e-02, -7.1800e-03,  1.0780e-01, -3.6197e-02,  5.7563e-02],\n",
      "          [-2.5874e-02,  2.9304e-02, -7.2387e-02, -4.9076e-02, -9.3789e-02],\n",
      "          [ 6.6420e-02,  1.7865e-02, -9.1774e-02, -1.4020e-02, -6.2903e-02]],\n",
      "\n",
      "         [[-8.0456e-02,  4.6762e-02, -9.6789e-04, -1.4725e-02, -9.2195e-02],\n",
      "          [ 2.4571e-03, -2.4973e-02, -1.8497e-02, -4.2137e-02, -9.7149e-02],\n",
      "          [-9.8758e-02,  4.9012e-02,  1.1508e-01,  9.7598e-02, -3.1148e-02],\n",
      "          [ 7.7303e-02,  1.5493e-03,  1.1471e-01,  1.0036e-01,  1.1192e-01],\n",
      "          [-2.6700e-02, -5.5793e-02, -2.9912e-02, -9.4426e-03,  1.1728e-02]],\n",
      "\n",
      "         [[-9.0919e-02,  5.5369e-02, -6.3858e-02, -9.3779e-02, -2.1040e-02],\n",
      "          [-7.1525e-03, -2.7453e-02, -1.2099e-02, -6.9326e-02,  7.8147e-02],\n",
      "          [-7.2538e-02,  8.9775e-03, -1.7301e-02,  4.1880e-02, -4.6276e-02],\n",
      "          [-6.1091e-02,  5.1414e-02, -9.9036e-02,  6.0917e-02,  7.2449e-02],\n",
      "          [ 7.4476e-02,  1.0399e-01,  7.7208e-02,  1.0766e-01,  2.0852e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.5074e-02, -6.0719e-02, -8.2917e-02, -2.1605e-02, -1.0935e-01],\n",
      "          [-6.3153e-02,  9.4287e-02,  3.8138e-02,  4.7423e-02,  6.0208e-03],\n",
      "          [-1.2095e-02,  9.8817e-02,  1.1287e-03, -8.9891e-02, -8.0971e-02],\n",
      "          [ 4.3949e-02, -4.7505e-02, -8.4220e-02, -2.7271e-02,  1.0744e-01],\n",
      "          [-9.1056e-02, -8.9019e-02, -2.3413e-02,  5.4677e-02, -1.0056e-01]],\n",
      "\n",
      "         [[-1.0364e-02, -7.9214e-03,  4.8435e-02, -7.9601e-02,  1.3221e-02],\n",
      "          [-1.0330e-01, -3.3529e-02, -2.9877e-02, -6.1855e-02,  4.6496e-02],\n",
      "          [ 4.8770e-02, -9.0771e-02, -2.9550e-02, -4.4739e-02, -7.6353e-03],\n",
      "          [ 1.5725e-02,  8.3406e-02,  7.2398e-02, -7.3478e-02, -2.9849e-03],\n",
      "          [ 1.4274e-02,  2.5503e-02,  1.5049e-02,  6.8013e-02,  5.6627e-02]],\n",
      "\n",
      "         [[-8.0681e-03,  8.8728e-02,  1.9535e-02, -7.3720e-02,  5.7104e-02],\n",
      "          [-5.7817e-02,  8.6448e-02, -5.6894e-02,  3.9793e-02, -8.4148e-02],\n",
      "          [ 8.7448e-02, -2.7047e-02, -4.6225e-02, -9.4267e-02,  1.0369e-01],\n",
      "          [-3.0828e-02, -7.1164e-03, -4.2770e-02, -4.8141e-02,  5.2087e-02],\n",
      "          [ 5.3838e-02,  9.4779e-02, -1.0896e-02, -5.5541e-02,  5.4766e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 8.5864e-02, -9.0153e-02,  7.3481e-02, -5.7959e-02,  5.0781e-02],\n",
      "          [ 2.9990e-02, -6.1685e-02, -6.1297e-02, -1.0888e-01,  5.7464e-02],\n",
      "          [ 1.1097e-01,  8.7396e-02, -9.6369e-02,  9.8192e-02, -6.5941e-02],\n",
      "          [ 9.7304e-02, -1.9152e-02, -7.9232e-02,  3.1915e-02,  1.1162e-01],\n",
      "          [ 6.7859e-02,  3.4161e-02,  5.3666e-02,  4.2623e-02,  1.1479e-01]],\n",
      "\n",
      "         [[-5.5617e-02,  1.5022e-02,  1.0688e-01,  3.4009e-02, -4.5169e-02],\n",
      "          [-8.1449e-02, -1.1453e-01,  2.0362e-02, -2.0326e-02, -5.3736e-02],\n",
      "          [-7.8627e-02,  4.5531e-02, -6.2925e-02,  7.1066e-02, -1.1288e-01],\n",
      "          [-1.1305e-01,  5.9413e-04,  8.7214e-02,  5.0956e-02,  6.3539e-02],\n",
      "          [-1.1275e-01, -8.3257e-02,  2.3604e-02,  4.1043e-02, -8.6008e-02]],\n",
      "\n",
      "         [[-9.1652e-02, -4.7727e-02,  4.3170e-02,  1.0601e-01, -7.4071e-02],\n",
      "          [ 2.7710e-02,  1.0169e-01,  2.8646e-02, -3.8309e-03, -2.5370e-02],\n",
      "          [ 7.1599e-04, -8.2299e-02, -1.9664e-03,  1.9451e-02,  1.0081e-01],\n",
      "          [ 4.5133e-02, -7.7785e-02,  2.5922e-03, -6.5711e-03, -9.1900e-02],\n",
      "          [ 4.5317e-02,  2.9074e-02, -4.5209e-02, -9.0468e-02,  9.8202e-02]]]])), ('bias', tensor([-0.0758, -0.0558,  0.0394, -0.0448,  0.0399,  0.0674, -0.0590, -0.0546,\n",
      "        -0.0307,  0.0582]))])\n"
     ]
    }
   ],
   "source": [
    "print(conv_layer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer weight shape: torch.Size([10, 3, 5, 5])\n",
      "conv_layer bias shape: torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Get shapes of weight and bias tensors within conv_layer_2\n",
    "print(f\"conv_layer weight shape: {conv_layer.weight.shape}\")\n",
    "print(f\"conv_layer bias shape: {conv_layer.bias.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),# options are \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2)\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2) # stride will be same as kernel size \n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7, \n",
    "                      out_features=output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        # print(x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (block_1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (block_2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = TinyVGG(input_shape=1, \n",
    "                  hidden_units=10, \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
